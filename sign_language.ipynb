{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:16:31.025720Z",
     "start_time": "2025-12-13T18:16:29.321059Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install opencv-python numpy tensorflow scikit-learn matplotlib pillow pandas mediapipe ffmpeg-python",
   "id": "3dfc61253fbea114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: pillow in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (0.10.10)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.41.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: jax in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from mediapipe) (0.4.34)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: future in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: jaxlib<=0.4.34,>=0.4.34 in c:\\users\\kongm\\pycharmprojects\\signlanguage\\.venv3\\lib\\site-packages (from jax->mediapipe) (0.4.34)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:16:39.784141Z",
     "start_time": "2025-12-13T18:16:38.186151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ],
   "id": "50f7c3739b3e1829",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Collection",
   "id": "45d44d08cfdeb575"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:17:01.744910Z",
     "start_time": "2025-12-13T18:16:39.791237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "CLASSES = [\"Yes\", \"No\"]   # <-- Add more categories here\n",
    "SAVE_DIR = \"dataset\"\n",
    "VIDEO_COUNT = 10\n",
    "DURATION = 3\n",
    "PAUSE_DURATION = 3\n",
    "FPS = 30\n",
    "FRAME_WIDTH = 1280\n",
    "FRAME_HEIGHT = 720\n",
    "# -------------------------\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "cap.set(cv2.CAP_PROP_FPS, FPS)\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "print(\"Starting multi-category recording...\")\n",
    "\n",
    "for CLASS_NAME in CLASSES:\n",
    "\n",
    "    print(f\"\\n===== START CATEGORY: {CLASS_NAME} =====\")\n",
    "\n",
    "    # Create folders\n",
    "    class_folder = os.path.join(SAVE_DIR, CLASS_NAME)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "    keypoints_folder = os.path.join(class_folder, \"keypoints\")\n",
    "    os.makedirs(keypoints_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(1, VIDEO_COUNT + 1):\n",
    "\n",
    "        # CSV file for keypoints\n",
    "        csv_filename = f\"{CLASS_NAME}_{i}_keypoints.csv\"\n",
    "        csv_filepath = os.path.join(keypoints_folder, csv_filename)\n",
    "\n",
    "        csv_file = open(csv_filepath, \"w\", newline=\"\")\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Header\n",
    "        header = []\n",
    "        for hand in ['left', 'right']:\n",
    "            for j in range(21):\n",
    "                header += [f\"{hand}_x{j}\", f\"{hand}_y{j}\", f\"{hand}_z{j}\"]\n",
    "        header.append(\"label\")\n",
    "        csv_writer.writerow(header)\n",
    "\n",
    "        # Video Writer\n",
    "        video_filename = f\"{CLASS_NAME}_{i}.mp4\"\n",
    "        video_path = os.path.join(class_folder, video_filename)\n",
    "        fourcc = cv4 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(video_path, fourcc, FPS, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "        print(f\"\\nRecording {CLASS_NAME} - Video {i}/{VIDEO_COUNT}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        while time.time() - start_time < DURATION:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Process with MediaPipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(rgb_frame)\n",
    "\n",
    "            left_hand_kp = [(0, 0, 0)] * 21\n",
    "            right_hand_kp = [(0, 0, 0)] * 21\n",
    "\n",
    "            if result.multi_hand_landmarks and result.multi_handedness:\n",
    "                for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                    label = handedness.classification[0].label\n",
    "                    hand_kp = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "\n",
    "                    if label == \"Left\":\n",
    "                        left_hand_kp = hand_kp\n",
    "                    else:\n",
    "                        right_hand_kp = hand_kp\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Save keypoints\n",
    "            row = [coord for kp in left_hand_kp + right_hand_kp for coord in kp]\n",
    "            row.append(CLASS_NAME)\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "            # Overlay info\n",
    "            sec_left = int(DURATION - (time.time() - start_time) + 1)\n",
    "            cv2.putText(frame, f\"Category: {CLASS_NAME}\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Recording Video {i}/{VIDEO_COUNT}\", (10, 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Time Left: {sec_left}s\", (10, 120),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Show & write\n",
    "            cv2.imshow(\"Recorder\", frame)\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                print(\"Stopped by user.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                raise SystemExit()\n",
    "\n",
    "        out.release()\n",
    "        csv_file.close()\n",
    "\n",
    "        print(f\"Saved video: {video_path}\")\n",
    "        print(f\"Saved keypoints: {csv_filepath}\")\n",
    "\n",
    "        # Pause between videos\n",
    "        print(f\"Waiting {PAUSE_DURATION} seconds...\")\n",
    "        pause_start = time.time()\n",
    "        while time.time() - pause_start < PAUSE_DURATION:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            remaining = int(PAUSE_DURATION - (time.time() - pause_start))\n",
    "            cv2.putText(frame, f\"Next video in {remaining}s\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "            cv2.imshow(\"Recorder\", frame)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                raise SystemExit()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\nAll categories recorded successfully!\")\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-category recording...\n",
      "\n",
      "===== START CATEGORY: Yes =====\n",
      "\n",
      "Recording Yes - Video 1/10\n",
      "Saved video: dataset\\Yes\\Yes_1.mp4\n",
      "Saved keypoints: dataset\\Yes\\keypoints\\Yes_1_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Yes - Video 2/10\n",
      "Saved video: dataset\\Yes\\Yes_2.mp4\n",
      "Saved keypoints: dataset\\Yes\\keypoints\\Yes_2_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Yes - Video 3/10\n",
      "Saved video: dataset\\Yes\\Yes_3.mp4\n",
      "Saved keypoints: dataset\\Yes\\keypoints\\Yes_3_keypoints.csv\n",
      "Waiting 3 seconds...\n",
      "\n",
      "Recording Yes - Video 4/10\n",
      "Saved video: dataset\\Yes\\Yes_4.mp4\n",
      "Saved keypoints: dataset\\Yes\\keypoints\\Yes_4_keypoints.csv\n",
      "Waiting 3 seconds...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Preprocessing",
   "id": "2e8d133a20270740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:17:05.288816Z",
     "start_time": "2025-12-13T18:17:05.152529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "DATASET_DIR = \"dataset\"    # folder with category subfolders\n",
    "SEQUENCE_LENGTH = 30       # number of frames per sample\n",
    "# -------------------------\n",
    "\n",
    "def fix_sequence_length(sequence, target_len):\n",
    "    \"\"\"Pad or truncate keypoint sequence to a fixed length.\"\"\"\n",
    "    if len(sequence) > target_len:\n",
    "        return sequence[:target_len]\n",
    "    elif len(sequence) < target_len:\n",
    "        pad = np.zeros((target_len - len(sequence), sequence.shape[1]))\n",
    "        return np.vstack([sequence, pad])\n",
    "    return sequence\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through categories\n",
    "for category in os.listdir(DATASET_DIR):\n",
    "    kp_dir = os.path.join(DATASET_DIR, category, \"keypoints\")\n",
    "    if not os.path.isdir(kp_dir):\n",
    "        continue\n",
    "\n",
    "    for csv_file in os.listdir(kp_dir):\n",
    "        csv_path = os.path.join(kp_dir, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Remove label column\n",
    "        keypoints = df.iloc[:, :-1].values.astype(np.float32)\n",
    "\n",
    "        # Fix sequence length\n",
    "        keypoints = fix_sequence_length(keypoints, SEQUENCE_LENGTH)\n",
    "\n",
    "        X.append(keypoints)\n",
    "        y.append(category)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (samples, SEQUENCE_LENGTH, 126)\n",
    "print(\"y shape:\", y.shape)  # (samples,)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"y_onehot shape:\", y_onehot.shape)\n",
    "\n",
    "# Save preprocessed data\n",
    "np.save(\"X_keypoints.npy\", X)\n",
    "np.save(\"y_labels.npy\", y_onehot)\n",
    "print(\"Preprocessed data saved to X_keypoints.npy and y_labels.npy\")"
   ],
   "id": "81f819c577ab1d89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25, 30, 126)\n",
      "y shape: (25,)\n",
      "Classes: ['Hello' 'No' 'Yes']\n",
      "y_onehot shape: (25, 3)\n",
      "Preprocessed data saved to X_keypoints.npy and y_labels.npy\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Modeling",
   "id": "d52010923a03830b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:17:11.638637Z",
     "start_time": "2025-12-13T18:17:06.952515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.load(\"X_keypoints.npy\")      # shape: (samples, SEQUENCE_LENGTH, 126)\n",
    "y = np.load(\"y_labels.npy\")         # shape: (samples, num_classes)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Model definition\n",
    "# -------------------------\n",
    "SEQUENCE_LENGTH = X.shape[1]\n",
    "FEATURES = X.shape[2]\n",
    "NUM_CLASSES = y.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1D Convolutions over keypoints per frame\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(SEQUENCE_LENGTH, FEATURES)))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM for temporal information\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "checkpoint = ModelCheckpoint(\"sign_language_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# -------------------------\n",
    "# Train-test split\n",
    "# -------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# Train the model\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ],
   "id": "9d02b7e9ea188379",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25, 30, 126)\n",
      "y shape: (25, 3)\n",
      "WARNING:tensorflow:From C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 28, 64)            24256     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 26, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 13, 128)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 128)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98563 (385.01 KB)\n",
      "Trainable params: 98563 (385.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 1s - loss: 1.1084 - accuracy: 0.1875\n",
      "Epoch 1: val_accuracy improved from -inf to 0.40000, saving model to sign_language_model.h5\n",
      "2/2 [==============================] - 2s 444ms/step - loss: 1.1081 - accuracy: 0.2500 - val_loss: 1.0141 - val_accuracy: 0.4000\n",
      "Epoch 2/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9891 - accuracy: 0.5625\n",
      "Epoch 2: val_accuracy did not improve from 0.40000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0402 - accuracy: 0.4500 - val_loss: 0.9772 - val_accuracy: 0.4000\n",
      "Epoch 3/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0246 - accuracy: 0.3750\n",
      "Epoch 3: val_accuracy improved from 0.40000 to 0.60000, saving model to sign_language_model.h5\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1.0115 - accuracy: 0.4000 - val_loss: 0.9548 - val_accuracy: 0.6000\n",
      "Epoch 4/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9602 - accuracy: 0.5625\n",
      "Epoch 4: val_accuracy improved from 0.60000 to 0.80000, saving model to sign_language_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongm\\PycharmProjects\\SignLanguage\\.venv3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 51ms/step - loss: 0.9819 - accuracy: 0.5000 - val_loss: 0.9407 - val_accuracy: 0.8000\n",
      "Epoch 5/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9353 - accuracy: 0.5625\n",
      "Epoch 5: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9218 - accuracy: 0.5500 - val_loss: 0.9292 - val_accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9133 - accuracy: 0.5625\n",
      "Epoch 6: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9308 - accuracy: 0.6000 - val_loss: 0.9256 - val_accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9151 - accuracy: 0.5625\n",
      "Epoch 7: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9175 - accuracy: 0.6000 - val_loss: 0.9088 - val_accuracy: 0.6000\n",
      "Epoch 8/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9717 - accuracy: 0.7500\n",
      "Epoch 8: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8915 - accuracy: 0.8000 - val_loss: 0.8888 - val_accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9024 - accuracy: 0.6875\n",
      "Epoch 9: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.8671 - accuracy: 0.7000 - val_loss: 0.8583 - val_accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9144 - accuracy: 0.6250\n",
      "Epoch 10: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8826 - accuracy: 0.6000 - val_loss: 0.8178 - val_accuracy: 0.6000\n",
      "Epoch 11/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8630 - accuracy: 0.7500\n",
      "Epoch 11: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.8025 - accuracy: 0.8000 - val_loss: 0.7765 - val_accuracy: 0.6000\n",
      "Epoch 12/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7753 - accuracy: 0.8125\n",
      "Epoch 12: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7872 - accuracy: 0.8000 - val_loss: 0.7360 - val_accuracy: 0.6000\n",
      "Epoch 13/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7810 - accuracy: 0.7500\n",
      "Epoch 13: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.7642 - accuracy: 0.7500 - val_loss: 0.6740 - val_accuracy: 0.8000\n",
      "Epoch 14/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7879 - accuracy: 0.7500\n",
      "Epoch 14: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.7264 - accuracy: 0.8000 - val_loss: 0.6038 - val_accuracy: 0.8000\n",
      "Epoch 15/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7044 - accuracy: 0.7500\n",
      "Epoch 15: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6522 - accuracy: 0.8000 - val_loss: 0.5442 - val_accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6356 - accuracy: 0.7500\n",
      "Epoch 16: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6849 - accuracy: 0.7500 - val_loss: 0.5222 - val_accuracy: 0.8000\n",
      "Epoch 17/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6319 - accuracy: 0.8750\n",
      "Epoch 17: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6533 - accuracy: 0.8000 - val_loss: 0.5481 - val_accuracy: 0.8000\n",
      "Epoch 18/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5056 - accuracy: 0.8750\n",
      "Epoch 18: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5201 - accuracy: 0.9000 - val_loss: 0.6358 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6282 - accuracy: 0.7500\n",
      "Epoch 19: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5865 - accuracy: 0.7500 - val_loss: 0.6922 - val_accuracy: 0.6000\n",
      "Epoch 20/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6728 - accuracy: 0.6250\n",
      "Epoch 20: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6388 - accuracy: 0.7000 - val_loss: 0.6797 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6754 - accuracy: 0.8125\n",
      "Epoch 21: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6202 - accuracy: 0.8500 - val_loss: 0.6329 - val_accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5641 - accuracy: 0.7500\n",
      "Epoch 22: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5437 - accuracy: 0.8000 - val_loss: 0.5791 - val_accuracy: 0.8000\n",
      "Epoch 23/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5101 - accuracy: 0.8750\n",
      "Epoch 23: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4709 - accuracy: 0.9000 - val_loss: 0.5344 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4107 - accuracy: 0.9375\n",
      "Epoch 24: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4019 - accuracy: 0.9500 - val_loss: 0.4985 - val_accuracy: 0.8000\n",
      "Epoch 25/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3926 - accuracy: 0.8750\n",
      "Epoch 25: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3873 - accuracy: 0.8500 - val_loss: 0.4656 - val_accuracy: 0.8000\n",
      "Epoch 26/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3934 - accuracy: 0.8750\n",
      "Epoch 26: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3261 - accuracy: 0.9000 - val_loss: 0.4447 - val_accuracy: 0.8000\n",
      "Epoch 27/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2276 - accuracy: 0.9375\n",
      "Epoch 27: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3029 - accuracy: 0.9000 - val_loss: 0.4535 - val_accuracy: 0.8000\n",
      "Epoch 28/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2835 - accuracy: 0.9375\n",
      "Epoch 28: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2577 - accuracy: 0.9500 - val_loss: 0.5600 - val_accuracy: 0.8000\n",
      "Epoch 29/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2256 - accuracy: 0.9375\n",
      "Epoch 29: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2440 - accuracy: 0.9500 - val_loss: 0.5662 - val_accuracy: 0.8000\n",
      "Epoch 30/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3102 - accuracy: 0.9375\n",
      "Epoch 30: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2758 - accuracy: 0.9500 - val_loss: 0.4880 - val_accuracy: 0.8000\n",
      "Epoch 31/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1980 - accuracy: 0.9375\n",
      "Epoch 31: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2167 - accuracy: 0.9000 - val_loss: 0.5244 - val_accuracy: 0.8000\n",
      "Epoch 32/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1180 - accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1619 - accuracy: 0.9375\n",
      "Epoch 33: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2653 - accuracy: 0.9000 - val_loss: 0.5862 - val_accuracy: 0.8000\n",
      "Epoch 34/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1047 - accuracy: 1.0000\n",
      "Epoch 34: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1329 - accuracy: 0.9500 - val_loss: 0.6030 - val_accuracy: 0.8000\n",
      "Epoch 35/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1981 - accuracy: 0.9375\n",
      "Epoch 35: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2048 - accuracy: 0.9500 - val_loss: 0.6498 - val_accuracy: 0.8000\n",
      "Epoch 36/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0697 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 0.80000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:17:31.438635Z",
     "start_time": "2025-12-13T18:17:11.646356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "MODEL_PATH = \"sign_language_model.h5\"   # your trained model\n",
    "DATASET_DIR = \"dataset\"                 # folder used in preprocessing\n",
    "SEQUENCE_LENGTH = 30\n",
    "FRAME_WIDTH = 1280\n",
    "FRAME_HEIGHT = 720\n",
    "FPS = 30\n",
    "# -------------------------\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Dynamically get class names from dataset folder\n",
    "CLASSES = [d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))]\n",
    "CLASSES.sort()  # ensure consistent order\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "cap.set(cv2.CAP_PROP_FPS, FPS)\n",
    "\n",
    "# Sequence buffer\n",
    "sequence = []\n",
    "\n",
    "print(\"Starting real-time gesture test. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Extract keypoints\n",
    "    left_hand_kp = [(0, 0, 0)] * 21\n",
    "    right_hand_kp = [(0, 0, 0)] * 21\n",
    "\n",
    "    if result.multi_hand_landmarks and result.multi_handedness:\n",
    "        for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "            label = handedness.classification[0].label\n",
    "            hand_kp = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "            if label == \"Left\":\n",
    "                left_hand_kp = hand_kp\n",
    "            else:\n",
    "                right_hand_kp = hand_kp\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Flatten keypoints and add to sequence\n",
    "    keypoints = [coord for kp in left_hand_kp + right_hand_kp for coord in kp]\n",
    "    sequence.append(keypoints)\n",
    "\n",
    "    # Keep last SEQUENCE_LENGTH frames\n",
    "    if len(sequence) > SEQUENCE_LENGTH:\n",
    "        sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "\n",
    "    # Make prediction when sequence is full\n",
    "    if len(sequence) == SEQUENCE_LENGTH:\n",
    "        input_data = np.expand_dims(sequence, axis=0)  # shape: (1, SEQUENCE_LENGTH, 126)\n",
    "        prediction = model.predict(input_data, verbose=0)\n",
    "        class_id = np.argmax(prediction)\n",
    "        class_name = CLASSES[class_id]\n",
    "        confidence = prediction[0][class_id]\n",
    "        cv2.putText(frame, f\"{class_name} ({confidence*100:.1f}%)\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Real-Time Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ],
   "id": "cac3c842dbe6fc13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time gesture test. Press 'q' to quit.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3a833ff418976f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
